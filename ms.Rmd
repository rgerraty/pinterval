---
title: "Covered with P: The consequences of null hypothesis significance testing on point and interval estimates"
shorttitle: "P Coverage"
author: 
  - name: Raphael T. Gerraty*
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: Columbia University, 
            Psychology Department, 
            406 Schermerhorn, 
            1190 Amsterdam Avenue, 
            New York, NY 10027
    email: https://github.com/neurostorm
  - name: Matti Vuorre*
    affiliation: 1
    corresponding: no
    address: No address provided.
    email: https://github.com/neurostorm
affiliation:
  - id: 1
    institution: Columbia University, Department of Psychology

abstract: |
  Confidence intervals (CI) do not allow post-data inference on parameter values. 
  
note: |  
  The authors declare no conflicts of interest.
  
keywords: "confidence interval, NHST"

wordcount: "Short and sweet."

class: man
lang: american
figsintext: yes
lineno: yes
bibliography:
  - p-intervals.bib

output: 
    papaja::apa6_pdf:
        keep_tex: false
---

```{r setup, message = FALSE, warning = FALSE}
library(knitr)
library(papaja)
library(ggplot2)
library(dplyr)
library(grid)
apa_prepare_doc() # Prepare document for rendering
theme_set(theme_minimal() + theme(
    legend.position = 'none',
    panel.grid.minor = element_blank(),
    panel.margin=unit(0.1, "cm"),
    panel.border = element_rect(fill = NA,colour = "grey20", size=.65),
    axis.text = element_text(size=8),
    axis.ticks = element_line(size = .2),
    axis.ticks.length=unit(-0.1, "cm"), 
    axis.ticks.margin=unit(0.2, "cm")
    )
)
```

# Introduction

Scientists, psychological or otherwise, routinely use null hypothesis significance testing procedures (NHSTP) to move from data to conclusions---a practice whose applicability has been debated since its inception [@rozeboom_fallacy_1960; @krantz_null_1999]. Recently, concerns about the replicability and reliability of empirical findings [@open_science_collaboration_estimating_2015] have reignited questions about the validity of NHSTP as they are implemented and interpreted in practice [@gelman_statistical_2014].

One response to the growing concerns regarding the reliability of NHSTP has been an appeal to effect size and interval estimation in addition---or as replacement---to NHSTP test statistics [@cumming_new_2014]. For example, many journals in psychology and neuroscience now ask authors to include confidence intervals (CI) with their test statistics: "The problems that pervade NHST are avoided by the new statisticsâ€”effect sizes, confidence intervals[...]" [@eich_business_2014 p.3]. "Reports of values of _r_ must, like reports of means, be accompanied by appropriate confidence intervals." [@lindsay_replication_2015, p.2] We agree on the benefits of interval estimation over computing _p_-values and embrace these recommendations, but fear that turning to CIs in particular,  especially when paired with significance testing, will lead to unwarranted levels of confidence in the intervals reported. 

The use of confidence intervals as advertised above seems to be intended to communicate estimates of likely ranges of parameter values, although the common computations simply result in ranges of parameter values that would not be rejected by the very statistical test the CI is supposed to supplement. This approach is problematic from at least two perspectives. First, CIs do not support probability statements about parameters [@ohagan_dicing_2004; @morey_fallacy_2015]. Second, a single observed CI does not have coverage properties apart from either including or excluding the true parameter value. A (infinite) sequence of CIs, or the generating procedure, however, does have the property of containing the true parameter on X% of occasions a single CI is realized from the sequence. This property is severely compromised by the current practice of using CIs as a post-hypothesis-testing tool, as we show in this paper.

While the first of these perspectives is misunderstood by many researchers in psychology and other fields [@hoekstra_robust_2014; but see @miller_interpreting_2015 and @morey_continued_2015], we focus here on the second, which is more subtle and even less appreciated among practitioners. Even many textbooks in statistics endorse the view that the frequency coverage or "confidence" of a CI refers to the specific values obtained for a _particular_ interval. This is an incorrect interpretation, because an interval either does or does not contain the true value, and the "confidence level" refers to the procedure, but given its widespread acceptance it is important to describe specific examples in which such an interpretation produces misleading results. 

In this paper, we report an unappealing property of obtained confidence intervals for results that pass a significance test. Because the claim of confidence intervals is for the procedure to have a coverage proportion of the true parameter value equal to the nominal value (usually 95%), it is crucial that this claim is substantiated in its long-run property for a CI to be what it claims to be. We show that using confidence intervals _in addition_ to P values leads to an undesirable distortion of the coverage proportion. This is a direct result of the more general problem with interpreting any particular _obtained_ CI in terms of the frequency coverage of the procedure used to generate CIs, but we feel the specific case of significance thresholding on this interpretation is worth describing, given the pervasive reporting and interpretation of such intervals following NHSTP in a wide range of research areas. We demonstrate analytically and with simulations that the proportion of confidence intervals that are significant _and_ contain the true parameter in question is a function of the power of a statistical procedure. Given the low power of many psychological studies (cite), and the widespread belief that obtained confidence intervals' nominal coverage provides valid information about parameters of interest (see blank for recommendations along this line), this demonstration may be of use to practitioners who feel confused or even strongly about the benefits of confidence interval estimation in NHSTP.

# Methods

We performed a simulation study...

# Results

```{r simulation, fig.cap = "Simulated sampling distribution of effects, power = 0.65, ES = 1. The grey vertical line indicates the mean effect of all samples, and the red line the mean of significant samples. The simulated sampling distributions are shown as density curves on the top, and a sample of 100 CIs in the bottom panel."}
# default values (corresponds to power of ~0.65)
mu=1
sigma=2.5
n=36
alpha=.05

ppower = power.t.test(n=n, delta=mu, sd=sigma, sig.level=alpha, type="one.sample")
N <- 10000  # Number of samples
# generate N random samples of size n from normal distribution
set.seed(1)
bt <- replicate(rnorm(n=n, mean=mu, sd=sigma), n=N)
bt_means <- apply(bt, 2, mean)  # sample means
f_se <- function(x) {sd(x) / sqrt(n)}
bt_ses <- apply(bt, 2, f_se)  # sample standard errors

# generate bounds for 1-alpha confidence intervals for each sample
crit <- qt(1-alpha/2, df=n-1)
upper <- bt_means + crit*bt_ses
lower <- bt_means - crit*bt_ses
d <- data.frame(mean = bt_means,
                upper = upper,
                lower = lower,
                n = 1:N)
signif_mu_in_interval <- nrow(filter(d, 
                                     lower > 0, 
                                     lower <= mu, 
                                     upper >= mu)) / N
mu_in_interval <- nrow(filter(d, 
                               lower <= mu, 
                               upper >= mu)) / N

source("plots.R")

```

We simulated 10.000 one-sample t-tests (Figure 1), and plotted the resulting CIs coverage property for all samples, and significance-filtered samples (Figure 2.)

```{r power_vs_coverage, cache=F, fig.cap = "Coverage proportion of confidence intervals versus statistical power. The red line represents the nominal coverage proportion (95%), black line is the actual coverage proportion when CIs are conditioned on the result being significant."}
# plot power against coverage
alpha = .05
sigma = 2.5
mu = 1
f_se <- function(x) {sd(x) / sqrt(k)}
N <- 500
pvsc <- data.frame("pwr" = NA, # power 
                     "coverage" = NA, # nominal coverage (1-alpha)
                     "coverage_p" = NA) # coverage of significant intervals
k <- 10
for (i in 1:70) {
    pvsc[i, 1] <- power.t.test(
        n = k, 
        delta = mu, 
        sd = sigma, 
        sig.level = alpha, 
        alternative = "two.sided",
        type = "one.sample"
        )$power
    bt <- replicate(rnorm(n = k, mean = mu, sd = sigma), n = N)
    means <- apply(bt, 2, mean)  # sample means
    ses <- apply(bt, 2, f_se)  # sample standard errors
    # generate bounds for 1-alpha confidence intervals for each sample
    crit <- qt(1 - alpha/2, df = k-1)
    upper <- means + crit * ses
    lower <- means - crit * ses
    d <- data.frame(mean = means,
                upper = upper,
                lower = lower,
                n = 1:N)
    # this is a bit of a tautology; always .95
    pvsc[i, 2] <- nrow(filter(d, lower <= mu, upper >= mu)) / N
    pvsc[i, 3] <- nrow(filter(d, lower > 0 | upper<0, lower <= mu, upper >= mu)) / N
    k <- k + 1
}
# plot simulation results and "model" results
mutate(pvsc, 
       true_cvg_sig = pwr - .05/2,  # true coverage | significant result
       true_cvg_all = .95) %>%  # nominal coverage
    ggplot(aes(x=pwr)) +
    geom_point(aes(y=coverage), col="red", shape=1) +
    geom_line(aes(y=true_cvg_all), col="red") +
    geom_point(aes(y=coverage_p), col="black", shape=1) +
    geom_line(aes(y=true_cvg_sig), col="black") +
    scale_x_continuous("Statistical power", breaks = seq(.1, .9, .2)) +
    scale_y_continuous("Coverage proportion", breaks = seq(.1, .9, .2))
```

# Discussion

Here we show a pervasive bias in the paramaters and intervals passing a null hypothesis significance threshold. We don't know if this result is well known to statisticians, but from the perspective of practitioners, we found it suprising. This paper was motivated in part by the discussions with colleagues who were equally suprised by the biases induced by hypothesis testing, especially on interval estimation. The "significance filter" has been discussed previously [@gelman_statistical_2011], but to our knowledge there have been no discussions of the effect of this filter on the frequency properties of confidence intervals. 

We note that, while the issues discussed in this paper are related to questionable research practices as well as known issues in null hypothesis testing such as alpha inflation due to multiple comparisons, the biased point estimates and interval coverage for significant results we discuss here are present in expectation even for single tests. Thus this bias will be more severe for significant results which have been filtered through such processes, but...

# References


\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}
