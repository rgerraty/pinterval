---
title: "Covered in P: The consequences of null hypothesis significance testing on point and interval estimates"
shorttitle: "P stains"
author: 
  - name: Raphael T. Gerraty
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: Columbia University, 
            Psychology Department, 
            406 Schermerhorn, 
            1190 Amsterdam Avenue, 
            New York, NY 10027
    email: https://github.com/neurostorm
  - name: Matti Vuorre
    affiliation: 1
    corresponding: no
    address: No address provided.
    email: https://github.com/neurostorm
affiliation:
  - id: 1
    institution: Columbia University, Department of Psychology

abstract: |
  Confidence intervals (CI) do not allow post-data inference on parameter values. 
  
note: |  
  The authors declare no conflicts of interest.
  
keywords: "confidence interval, NHST"

wordcount: "Short and sweet."

class: man
lang: american
figsintext: no
lineno: yes
bibliography:
  - p-intervals.bib

output: papaja::apa6_pdf
---

```{r setup, message = FALSE, warning = FALSE}
library(papaja)
library(ggplot2)
library(dplyr)
library(grid)
apa_prepare_doc() # Prepare document for rendering
theme_set(theme_minimal() + theme(
    legend.position = 'none',
    panel.grid = element_blank(),
    panel.margin=unit(0.1, "cm"),
    panel.border = element_rect(fill = NA,colour = "grey20", size=.65),
    axis.text = element_text(size=8),
    axis.ticks = element_line(size = .2),
    axis.ticks.length=unit(-0.1, "cm"), 
    axis.ticks.margin=unit(0.2, "cm")
    )
)
```

# Introduction

<!--- This should be an introduction to NHSTP, its problems, and the subsequent suggestions that CIs will save our butts. --->

Scientists, psychological or otherwise, routinely use null hypothesis significance testing procedures (NHSTP) to move from data to conclusions---a practice thats applicability has been debated since its inception. Recently, concerns about the replicability and reliability of empirical findings [@open_science_collaboration_estimating_2015] have underlined the concerns about NHSTP as _the_ valid form of statistical inference [@cumming_new_2014; @gelman_statistical_2014].

One response to the growing concerns regarding the reliability of NHSTP has been an appeal to effect size and interval estimation in addition---or as replacement---to NHSTP test statistics [@cumming_new_2014]. For example, many journals in psychology and neuroscience now ask authors to include _confidence intervals_ (CI) with their test statistics. These confidence intervals are intended, by practitioners, to communicate unbiased estimates of likely ranges of parameter values, although the common computations simply result in ranges of parameter values that would not be rejected by the very statistical test the CI is supposed to supplement. This approach is problematic from at least two perspectives:

* CIs do not support probability statements about parameters
* CIs are designed (Neyman, 1957) to be a procedure of generating a set of intervals that, as a whole set, contain the true parameter value X% of the time
    * This property is severely compromised by the current practice of using CIs as a post-data inferential tool, as we show in this paper

## A Confidence Interval

<!--- Here we briefly cite a dozen statistics textbooks misusing confidence intervals [and cite the band of bayesians work on this; http://bayesfactor.blogspot.co.uk/2015/12/confidence-intervals-what-they-are-and.html], and give the correct definition of a confidence interval. We should also point out that there's no confidence, in principle, in confidence intervals, and maybe note bayesian intervals? --->

While the first of these interpretations is inaccurately held by many researchs in psychology and other fields (citing stuff), 

In this paper, we report an unappealing property of confidence intervals. Because the claim of confidence intervals is to have a coverage proportion of the true parameter value equal to the nominal value (usually 95%), it is crucial that this claim is substantiated in its long-run property for a CI to be what it claims to be (not a _confidence_ interval.) We show that using confidence intervals _in addition_ to P values leads to an undesirable distortion of the coverage proportion. This is a direct result of the more general problem with interpreting any particular _obtained_ CI in terms of frequency coverage, but we feel the specific case of significance thresholding on this interpretation is worth describing, given the per

## P stains the nominal coverage proportion

<!--- Here we describe the gist of the paper. --->

# Methods

We performed a simulation study...

# Results

```{r simulation}
# default values (corresponds to power of ~0.65)
mu=1
sigma=2.5
n=36
alpha=.05

ppower = power.t.test(n=n, delta=mu, sd=sigma, sig.level=alpha, type="one.sample")
ppower$power
N <- 1000  # Number of samples
# generate N random samples of size n from normal distribution
set.seed(1)
bt <- replicate(rnorm(n=n, mean=mu, sd=sigma), n=N)
bt_means <- apply(bt, 2, mean)  # sample means
f_se <- function(x) {sd(x) / sqrt(n)}
bt_ses <- apply(bt, 2, f_se)  # sample standard errors

# generate bounds for 1-alpha confidence intervals for each sample
crit <- qt(1-alpha/2, df=n-1)
upper <- bt_means + crit*bt_ses
lower <- bt_means - crit*bt_ses
d <- data.frame(mean = bt_means,
                upper = upper,
                lower = lower,
                n = 1:N)
signif_mu_in_interval <- nrow(filter(d, 
                                     lower > 0, 
                                     lower <= mu, 
                                     upper >= mu)) / N
mu_in_interval <- nrow(filter(d, 
                               lower <= mu, 
                               upper >= mu)) / N
```

```{r power_vs_coverage, cache=T}
# plot power againt coverage
alpha = .05
sigma = 2.5
mu = 1
f_se <- function(x) {sd(x) / sqrt(k)}
N <- 500
pvsc <- data.frame("pwr" = NA, # power 
                     "coverage" = NA, # nominal coverage (1-alpha)
                     "coverage_p" = NA) # coverage of significant intervals
k <- 10
for (i in 1:70) {
    pvsc[i, 1] <- power.t.test(
        n = k, 
        delta = mu, 
        sd = sigma, 
        sig.level = alpha, 
        type = "one.sample"
        )$power
    bt <- replicate(rnorm(n = k, mean = mu, sd = sigma), n = N)
    means <- apply(bt, 2, mean)  # sample means
    ses <- apply(bt, 2, f_se)  # sample standard errors
    # generate bounds for 1-alpha confidence intervals for each sample
    crit <- qt(1 - alpha/2, df = k-1)
    upper <- means + crit * ses
    lower <- means - crit * ses
    d <- data.frame(mean = means,
                upper = upper,
                lower = lower,
                n = 1:N)
    # this is a bit of a tautology; always .95
    pvsc[i, 2] <- nrow(filter(d, lower <= mu, upper >= mu)) / N
    pvsc[i, 3] <- nrow(filter(d, lower > 0, lower <= mu, upper >= mu)) / N
    k <- k + 1
}
pvsc <- mutate(pvsc, bias = coverage - coverage_p)
ggplot(pvsc, aes(x=pwr)) +
    geom_point(aes(y=coverage), col="red") +
    geom_point(aes(y=coverage_p), col="black") +
    scale_x_continuous(breaks = seq(.1, .9, .2)) +
    scale_y_continuous(breaks = seq(.1, .9, .2))
```

# Discussion

Here we show a pervasive bias in the paramaters and intervals passing a null hypothesis significance threshold. We don't know if this result is well known to statisticians, but from the perspective of practitioners, we found it suprising. This paper was motivated in part by the discussions with colleagues who were equally suprised by the biases induced by hypothesis testing, especially on interval estimation. The "significance filter" has been discussed previously [@gelman_statistical_2011], but to our knowledge there have been no discussions of the effect of this filter on the frequency properties of confidence intervals. 

We note that, while the issues discussed in this paper are related to questionable research practices as well as known issues in null hypothesis testing such as alpha inflation due to multiple comparisons, the biased point estimates and interval coverage for significant results we discuss here are present in expectation even for single tests. Thus this bias will be more severe for significant results which have been filtered through such processes, but...

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}
